---
title: 主题模型
date: 2020-09-21 13:38:41
tags: 
    - Topic Model
    - 主题模型
categories: 
    - Topic Model
    - 主题模型
abbrlink: '20200921-TopicModel'
---

# 主题模型 （Topic Model）

* **主题模型**（Topic Model）是以无监督学习的方式对文档的隐含语义结构(latent semantic structure)进行聚类(clustering)的统计模型。

> 主题模型认为在词(word)与文档(document)之间没有直接的联系，应当还有一个维度将它们串联起来，这个维度称为主题(topic)。

每个文档都对应着一个或多个主题，而每个主题都有对应的词分布，通过主题，就可以得到每个文档的词分布。

由此有公式：

$$p(\omega_i | d_j) = \sum _{k=1} ^{K} p(\omega_i | t_k) \times p( t_k | d_j), 
其中\omega表示词，d表示文档，t表示主题，K表示主题个数$$

> 在一个已知的数据集中，词和文档对应的$p(\omega_i | d_j)$都是已知的。主题模型就是根据这个已知的信息，通过计算$p(\omega_i | t_k)$和$p( t_k | d_j)$，从而得到**主题的词分布**和**文档的主题分布**信息。

常用方法
* LSA（Latent Semantic Analysis）
    * 主要采用SVD（奇异值分解）暴力破解
* LDA（Latent Dirichlet Allocation, 隐含狄利克雷分布）
    * 贝叶斯学派方法论进行拟合

# LSA（Latent Semantic Analysis）

LSA最初用在语义检索上，为解决**一词多义**和**一义多词**的问题：
* 一义多词： 
> 美女和PPMM表示相同的含义，但是单纯依靠检索词“美女”来检索文档，很可能丧失掉那些包含“PPMM”的文档。
* 一词多义：
> 如果输入检索词是多个检索词组成的一个小document，例如“清澈 孩子”，那我们就知道这段文字主要想表达concept是和道德相关的，不应该将“春天到了，小河多么的清澈”这样的文本包含在内。

为了能够解决这个问题，需要将词语（term）中的concept提取出来，建立一个词语和概念的关联关系（t-c relationship），这样一个文档就能表示成为概念的向量。这样输入一段检索词之后，就可以先将检索词转换为概念，再通过概念去匹配文档

# LDA (Latent Dirichlet Allocation)

**隐含狄利克雷分布**（Latent Dirichlet Allocation, LDA）是由David Blei等人在**2003**年提出的，该方法的理论基础是**贝叶斯理论**。

> LDA根据**词的共现信息**，拟合出词-文档-主题的分布，进而将词、文本都映射到一个语义空间中。

LDA算法假设文档中主题的先验分布和主题中词的先验分布都服从狄利克雷分布。在贝叶斯学派看来，先验分布+数据(似然)=后验分布。我们通过对已有数据集的统计，就可以得到每篇文档中主题的多项式分布和每个主题对应词的多项式分布。然后就可以根据贝叶斯学派的方法，通过先验的狄利克雷分布和观测数据得到的多项式分布，得到一组Dirichlet-multi共轭，并据此来推断文档中主题的后验分布，也就是我们最后需要的结果。那么具体的LDA模型应当如何进行求解，其中一种主流的方法就是吉布斯采样。结合吉布斯采样的LDA模型训练过程一般如下：

随机初始化，对语料中每篇文档中的每个词w，随机地赋予一个topic编号z。
重新扫描语料库，对每个词w按照吉布斯采样公式重新采样它的topic，在语料中进行更新。
重复以上语料库的重新采样过程直到吉布斯采样收敛。
统计语料库的topic-word共现频率矩阵，该矩阵就是LDA的模型。
经过以上的步骤，就得到一个训练好的LDA模型，接下来就可以按照一定的方式针对新文档的topic进行预估，具体步骤如下：

随机初始化，对当前文档中的每个词w，随机地赋予一个topic编号z。
重新扫描当前文档，按照吉布斯采样公式，重新采样它的topic。
重复以上过程直到吉布斯采样收敛。
统计文档中的topic分布即为预估结果。







